{"cells":[{"cell_type":"markdown","metadata":{"id":"KG071bmtVFFf"},"source":["# **Noisy Double DQN Reinforcement Learning Agent for Blackjack**\n","In this notebook, we evaluate the noisey trained model using the NoisyDoubleDQN class within our training notebook. In our training, we used 500,000 epochs to train the agent using off-policy learning with a replaymemory."]},{"cell_type":"markdown","metadata":{"id":"AtUvlSMpqHc-"},"source":["## Imports and Installs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ancPCGb9eX9A"},"outputs":[],"source":["%%capture\n","# capture line is to hide the output\n","# Install required packages\n","!pip install gymnasium torch numpy matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLMCIhrzSIyj"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import random\n","import math\n","import random\n","from collections import namedtuple, deque"]},{"cell_type":"code","source":["import gdown"],"metadata":{"id":"UjlffBgIrpP5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lum8T1EmWGpY"},"source":["## **Noisy Double DQN Agent with Replay Memory Implementation**\n","The Agent and the ReplayMemory class are instantiated here, and then the model weights are loaded. For additional improvement, it might be beneficial to save the agent and replaymemory as a class within a .py file for import."]},{"cell_type":"code","source":["# Define a named tuple to store experiences\n","Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n","\n","class ReplayMemory:\n","    def __init__(self, capacity=10000):\n","        self.memory = deque(maxlen=capacity)\n","        self.capacity = capacity\n","\n","    def push(self, state, action, reward, next_state, done):\n","        \"\"\"Save an experience to memory\"\"\"\n","        experience = Experience(state, action, reward, next_state, done)\n","        self.memory.append(experience)\n","\n","    def sample(self, batch_size):\n","        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n","        if batch_size > len(self.memory):\n","            batch_size = len(self.memory)\n","        experiences = random.sample(self.memory, batch_size)\n","\n","        # Convert to separate arrays\n","        states = torch.FloatTensor([exp.state for exp in experiences])\n","        actions = torch.LongTensor([exp.action for exp in experiences])\n","        rewards = torch.FloatTensor([exp.reward for exp in experiences])\n","        next_states = torch.FloatTensor([exp.next_state for exp in experiences])\n","        dones = torch.FloatTensor([exp.done for exp in experiences])\n","\n","        return states, actions, rewards, next_states, dones\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","# Initialize replay memory\n","memory = ReplayMemory()\n","\n","# Test the replay memory with a sample experience\n","sample_experience = (\n","    np.array([11, 9, 0]),  # state\n","    1,                     # action (hit)\n","    0,                     # reward\n","    np.array([16, 9, 0]),  # next_state\n","    False                  # not done\n",")\n","\n","memory.push(*sample_experience)\n","print(f\"Memory size after pushing one experience: {len(memory)}\")\n","\n","# Test sampling (this will only return the one experience we pushed)\n","states, actions, rewards, next_states, dones = memory.sample(1)\n","print(\"\\nSampled experience:\")\n","print(f\"State: {states}\")\n","print(f\"Action: {actions}\")\n","print(f\"Reward: {rewards}\")\n","print(f\"Next State: {next_states}\")\n","print(f\"Done: {dones}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YA6hb_lhqW7X","executionInfo":{"status":"ok","timestamp":1732762876578,"user_tz":480,"elapsed":5,"user":{"displayName":"Bryan Alexis Ambriz","userId":"16154433038435291108"}},"outputId":"82626c74-406b-4a8d-de44-662f9b4f350e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Memory size after pushing one experience: 1\n","\n","Sampled experience:\n","State: tensor([[11.,  9.,  0.]])\n","Action: tensor([1])\n","Reward: tensor([0.])\n","Next State: tensor([[16.,  9.,  0.]])\n","Done: tensor([0.])\n"]}]},{"cell_type":"code","source":["class NoisyLinear(nn.Module):\n","    \"\"\"Noisy linear module for NoisyNet.\n","\n","    Attributes:\n","        in_features (int): input size of linear module\n","        out_features (int): output size of linear module\n","        std_init (float): initial std value\n","        weight_mu (nn.Parameter): mean value weight parameter\n","        weight_sigma (nn.Parameter): std value weight parameter\n","        bias_mu (nn.Parameter): mean value bias parameter\n","        bias_sigma (nn.Parameter): std value bias parameter\n","\n","    \"\"\"\n","\n","    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):\n","        \"\"\"Initialization.\"\"\"\n","        super(NoisyLinear, self).__init__()\n","\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.std_init = std_init\n","\n","        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.weight_sigma = nn.Parameter(\n","            torch.Tensor(out_features, in_features)\n","        )\n","        self.register_buffer(\n","            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n","        )\n","\n","        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n","        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n","        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n","\n","        self.reset_parameters()\n","        self.reset_noise()\n","\n","    def reset_parameters(self):\n","        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n","        mu_range = 1 / math.sqrt(self.in_features)\n","        self.weight_mu.data.uniform_(-mu_range, mu_range)\n","        self.weight_sigma.data.fill_(\n","            self.std_init / math.sqrt(self.in_features)\n","        )\n","        self.bias_mu.data.uniform_(-mu_range, mu_range)\n","        self.bias_sigma.data.fill_(\n","            self.std_init / math.sqrt(self.out_features)\n","        )\n","\n","    def reset_noise(self):\n","        \"\"\"Make new noise.\"\"\"\n","        epsilon_in = self.scale_noise(self.in_features)\n","        epsilon_out = self.scale_noise(self.out_features)\n","\n","        # outer product\n","        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n","        self.bias_epsilon.copy_(epsilon_out)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward method implementation.\n","\n","        We don't use separate statements on train / eval mode.\n","        It doesn't show remarkable difference of performance.\n","        \"\"\"\n","        return F.linear(\n","            x,\n","            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n","            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n","        )\n","\n","    @staticmethod\n","    def scale_noise(size: int) -> torch.Tensor:\n","        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n","        x = torch.randn(size)\n","\n","        return x.sign().mul(x.abs().sqrt())"],"metadata":{"id":"Q6BMvE8cqd2m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Noisy DDQN Architecture"],"metadata":{"id":"jvfum1AEaLo3"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","class NoisyDoubleDQNAgent:\n","    def __init__(self, input_dim=3, learning_rate=5e-4, gamma=0.99, epsilon=1.0):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.fitness = [] # used to store a series of 'avg reward' during evaluation\n","\n","        self.policy_noisy_linear1 = NoisyLinear(128, 64)\n","        self.policy_noisy_linear2 = NoisyLinear(64, 2)\n","        self.target_noisy_linear1 = NoisyLinear(128, 64)\n","        self.target_noisy_linear2 = NoisyLinear(64, 2)\n","\n","        # Larger network\n","        self.policy_net = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            self.policy_noisy_linear1, # the noisy layers w/o ReLU in-between\n","            self.policy_noisy_linear2,\n","        ).to(self.device)\n","\n","        self.target_net = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            self.target_noisy_linear1,\n","            self.target_noisy_linear2\n","        ).to(self.device)\n","\n","        self.target_net.load_state_dict(self.policy_net.state_dict())\n","\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n","        self.memory = ReplayMemory(capacity=20000)\n","\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.epsilon_min = 0.05\n","        self.epsilon_decay = 0.99997\n","        self.batch_size = 128\n","        self.target_update = 10\n","        self.episode_count = 0\n","\n","\n","    def reset_noise(self):\n","        \"\"\"Reset all noisy layers.\"\"\"\n","        self.policy_noisy_linear1.reset_noise()\n","        self.policy_noisy_linear2.reset_noise()\n","        self.target_noisy_linear1.reset_noise()\n","        self.target_noisy_linear2.reset_noise()\n","\n","    def select_action(self, state):\n","        \"\"\"Select action using epsilon-greedy policy\"\"\"\n","        if random.random() < self.epsilon:\n","            return random.randint(0, 1)\n","\n","        with torch.no_grad():\n","            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n","            q_values = self.policy_net(state)\n","            return q_values.argmax().item()\n","\n","    def update_epsilon(self):\n","        \"\"\"Decay epsilon value\"\"\"\n","        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n","\n","    def store_transition(self, state, action, reward, next_state, done):\n","        \"\"\"Store transition in replay memory\"\"\"\n","        self.memory.push(state, action, reward, next_state, done)\n","\n","    def train_step(self):\n","        \"\"\"Perform one training step\"\"\"\n","        if len(self.memory) < self.batch_size:\n","            return\n","\n","        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n","        states = states.to(self.device)\n","        actions = actions.to(self.device)\n","        rewards = rewards.to(self.device)\n","        next_states = next_states.to(self.device)\n","        dones = dones.to(self.device)\n","\n","        # Double DQN implementation\n","        with torch.no_grad():\n","            next_actions = self.policy_net(next_states).argmax(1)\n","            next_q_values = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n","            target_q_values = rewards + (1 - dones.float()) * self.gamma * next_q_values\n","\n","        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n","\n","        # Huber loss for better stability\n","        loss = nn.SmoothL1Loss()(current_q_values, target_q_values)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n","        self.optimizer.step()\n","\n","        # reset the noisy layers\n","        self.reset_noise()\n","\n","        return loss.item()\n","\n","    def update_target_network(self):\n","        \"\"\"Update target network parameters\"\"\"\n","        if self.episode_count % self.target_update == 0:\n","            self.target_net.load_state_dict(self.policy_net.state_dict())\n","        self.episode_count += 1\n","\n","# Initialize the improved agent\n","agent = NoisyDoubleDQNAgent()\n","\n","# Modified training function\n","def train_agent(num_episodes=2000, print_interval=100):\n","    rewards = []\n","    losses = []\n","    epsilons = []\n","\n","    for episode in range(num_episodes):\n","        state, _ = env.reset()\n","        episode_reward = 0\n","        done = False\n","\n","        while not done:\n","            # Select and perform action\n","            action = agent.select_action(state)\n","            next_state, reward, done, truncated, _ = env.step(action)\n","\n","            # Store transition\n","            agent.store_transition(state, action, reward, next_state, done)\n","\n","            # Move to next state\n","            state = next_state\n","            episode_reward += reward\n","\n","            # Perform training step\n","            loss = agent.train_step()\n","            if loss is not None:\n","                losses.append(loss)\n","\n","        # Update target network\n","        agent.update_target_network()\n","\n","        # Update exploration rate\n","        agent.update_epsilon()\n","\n","        # Store metrics\n","        rewards.append(episode_reward)\n","        epsilons.append(agent.epsilon)\n","\n","        # Print progress\n","        if (episode + 1) % print_interval == 0:\n","            avg_reward = np.mean(rewards[-print_interval:])\n","            avg_loss = np.mean(losses[-print_interval:]) if losses else 0\n","            print(f\"Episode {episode + 1}\")\n","            print(f\"Average Reward: {avg_reward:.3f}\")\n","            print(f\"Average Loss: {avg_loss:.3f}\")\n","            print(f\"Epsilon: {agent.epsilon:.3f}\")\n","            print(\"-\" * 50)\n","\n","    return rewards, losses, epsilons"],"metadata":{"id":"pou_nk-9qifk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading the Trained Q-Policy Model Weights"],"metadata":{"id":"5FTIwuX2tS8a"}},{"cell_type":"markdown","source":["The policy net state dictionary is the most important aspect of the training of the model. The target network is less useful as it learns without consideration of the states themselves, and is mostly used during training - whereas the policy net is  effective at inference."],"metadata":{"id":"rJbOR1UZ6Fx-"}},{"cell_type":"code","source":["# downloading the .pth file from the google drive link using gdown and the id\n","filename='Blackjack_Noisy_DoubledQN.pth'\n","model_weights_link = 'https://drive.google.com/file/d/17sW4uL7M4MbUPSGnrSYjWiJC-1N1l4qd/view?usp=drive_link'\n","model_weights_id = model_weights_link.split('/')[5]\n","gdown.download(output=filename,id = model_weights_id, quiet=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"9Ztg8CP5rgMT","executionInfo":{"status":"ok","timestamp":1732762880524,"user_tz":480,"elapsed":3949,"user":{"displayName":"Bryan Alexis Ambriz","userId":"16154433038435291108"}},"outputId":"68419841-488a-4f7c-9bdf-fc61494e4c9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=17sW4uL7M4MbUPSGnrSYjWiJC-1N1l4qd\n","To: /content/Blackjack_Noisy_DoubledQN.pth\n","100%|██████████| 455k/455k [00:00<00:00, 34.4MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["'Blackjack_Noisy_DoubledQN.pth'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1732762880524,"user":{"displayName":"Bryan Alexis Ambriz","userId":"16154433038435291108"},"user_tz":480},"id":"6ZGDNPckWZ0I","outputId":"9fbcfd81-16f6-4e22-ed7b-5464c7596792"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-19-da1497d05a62>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  NoisyDDQN.policy_net.load_state_dict(torch.load(f'/content/{filename}')['policy_net_state_dict'])\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":19}],"source":["# Initialize the improved agent then loading the state dict from the .pth file\n","NoisyDDQN = NoisyDoubleDQNAgent()\n","NoisyDDQN.policy_net.load_state_dict(torch.load(f'/content/{filename}')['policy_net_state_dict'])"]},{"cell_type":"code","source":["NoisyDDQN.to_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"djBahj16tjM2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcpUXf7qa47U"},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{"id":"4aUHVllYjxmR"},"source":["### Evaluation Metrics of the Trained Model on a New Environment\n","We create a new test environment using the RGB array with Gymnasium to visualize the agent's environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_EumUs-fk_8"},"outputs":[],"source":["test_env = gym.make(\"Blackjack-v1\", render_mode='rgb_array')"]},{"cell_type":"markdown","source":["Our evaluation function removes some components that were used in the training function and mostly captures the rewards - such as whether the agent was able to play optimally in terms of the number of wins, draws, and losses."],"metadata":{"id":"r-5qRlHe6mG8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1314,"status":"ok","timestamp":1732762938477,"user":{"displayName":"Bryan Alexis Ambriz","userId":"16154433038435291108"},"user_tz":480},"id":"p4Z1zz0vikvC","outputId":"412d809c-b99d-4994-b8be-ddd0b602a238"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating trained agent...\n","\n","Detailed Evaluation Results:\n","Number of Episodes: 200\n","Win Rate: 44.0%  (88/200)\n","Draw Rate: 8.0%  (16/200)\n","Loss Rate: 48.0%  (96/200)\n","Average Reward: -0.040\n","Average Final Player Sum: 19.7\n"]}],"source":["# Evaluate the trained agent\n","def evaluate_agent_detailed(test_env, trained_agent, n_episodes=200):\n","    wins = 0\n","    draws = 0\n","    losses = 0\n","    total_rewards = []\n","    player_sums = []\n","    dealer_sums = []\n","\n","    for episode in range(n_episodes):\n","        state, _ = test_env.reset()\n","        done = False\n","        episode_reward = 0\n","\n","        while not done:\n","            # Use greedy policy (no exploration)\n","            with torch.no_grad():\n","                # Move state_tensor to the same device as the policy_net\n","                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(trained_agent.device)\n","                action = trained_agent.policy_net(state_tensor).argmax().item()\n","\n","            state, reward, done, truncated, _ = test_env.step(action)\n","            episode_reward += reward\n","\n","        total_rewards.append(episode_reward)\n","        if reward > 0:\n","            wins += 1\n","        elif reward == 0:\n","            draws += 1\n","        else:\n","            losses += 1\n","\n","        player_sums.append(state[0])  # Final player sum\n","\n","    print(\"\\nDetailed Evaluation Results:\")\n","    print(f\"Number of Episodes: {n_episodes}\")\n","    print(f\"Win Rate: {wins/n_episodes*100:.1f}%  ({wins}/{n_episodes})\")\n","    print(f\"Draw Rate: {draws/n_episodes*100:.1f}%  ({draws}/{n_episodes})\")\n","    print(f\"Loss Rate: {losses/n_episodes*100:.1f}%  ({losses}/{n_episodes})\")\n","    print(f\"Average Reward: {np.mean(total_rewards):.3f}\")\n","    print(f\"Average Final Player Sum: {np.mean(player_sums):.1f}\")\n","\n","    return total_rewards, player_sums\n","\n","# Evaluate the trained agent\n","print(\"\\nEvaluating trained agent...\")\n","eval_rewards, player_sums = evaluate_agent_detailed(test_env, NoisyDDQN, n_episodes=200)"]},{"cell_type":"markdown","metadata":{"id":"y2QJAaa2fvcc"},"source":["### Visualizing the Agent's Learning with Gymnasium's RGB Environment\n","This visualization is less useful for evaluation and is mostly useful for interpretting the gameplay. The evaluation output is informative and tells us the statistics of how the agent fares within the evaluation environment. We found that our agent usually performs within the 40-50% win range. This is quite normal as blackjack is at most a game of luck, and in general even the optimal stategy has a .05% disadvantage compared to the dealer/the antagonist of the game."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1357,"status":"ok","timestamp":1732762958807,"user":{"displayName":"Bryan Alexis Ambriz","userId":"16154433038435291108"},"user_tz":480},"id":"AwyJyBPkfl39","outputId":"2da79124-1087-43b7-b483-d84434707876"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/gym_monitor_output folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n","  logger.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Detailed Evaluation Results:\n","Number of Episodes: 200\n","Win Rate: 35.0%  (70/200)\n","Draw Rate: 16.0%  (32/200)\n","Loss Rate: 49.0%  (98/200)\n","Average Reward: -0.140\n","Average Final Player Sum: 19.9\n"]}],"source":["# Uses the Gym Monitor wrapper to evalaute the agent and record video\n","# only one video will be saved\n","# video of the final episode with the episode trigger\n","test_env = gym.wrappers.RecordVideo(\n","    test_env, \"./gym_monitor_output\", episode_trigger=lambda x: x == 0)\n","\n","evaluate_agent_detailed(test_env, NoisyDDQN)\n","\n","test_env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PlJGfS08nMwd"},"outputs":[],"source":["# play a video using a path to the video\n","from IPython.display import Video\n","from base64 import b64encode\n","\n","def show_video(video_path):\n","    video_file = Video(video_path, embed=True)\n","    display(video_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1732762966024,"user":{"displayName":"Bryan Alexis Ambriz","userId":"16154433038435291108"},"user_tz":480},"id":"oMwgylKlnaDn","outputId":"cd2d6bbd-5073-4816-ae03-7b69882e20a9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Video object>"],"text/html":["<video controls  >\n"," <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n"," Your browser does not support the video tag.\n"," </video>"]},"metadata":{}}],"source":["# visualizing the rl-video-episode-0.mp4 in the gym_monitor_output\n","show_video(\"./gym_monitor_output/rl-video-episode-0.mp4\")"]},{"cell_type":"markdown","metadata":{"id":"lpSdmCd8jOY_"},"source":["## Strategy Analysis and Interactive Human Feedback Loop with A.I Recommendation\n","Finally, we implemented a stategy analysis to understand the kinds of decisions that were learned by the agent within the Q-Learning policy matrix. We also implemented a human feedback loop with the agent's action recommendations. The training loop retrains the model after each game and helps to train the model using the human's actions."]},{"cell_type":"markdown","metadata":{"id":"BcTSx1uqjaNj"},"source":["### Strategy Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1732763028525,"user":{"displayName":"Bryan Alexis Ambriz","userId":"16154433038435291108"},"user_tz":480},"id":"6m6LhAQLfZCX","outputId":"74b82e3c-ca2e-4a9c-cb5b-69c6af93e670"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Strategy Analysis:\n","Player Sum | Dealer Card | Action\n","-----------------------------------\n","    16     |     10      |  Hit  \n","    12     |      6      |  Hit  \n","    18     |      9      | Stand \n","    11     |     10      |  Hit  \n","    15     |      7      |  Hit  \n"]}],"source":["def analyze_strategy(agent):\n","    # Common situations in blackjack\n","    test_states = [\n","        (16, 10, 0),  # Hard 16 vs dealer 10\n","        (12, 6, 0),   # Hard 12 vs dealer 6\n","        (18, 9, 0),   # Hard 18 vs dealer 9\n","        (11, 10, 0),  # Hard 11 vs dealer 10\n","        (15, 7, 0),   # Hard 15 vs dealer 7\n","    ]\n","\n","    print(\"\\nStrategy Analysis:\")\n","    print(\"Player Sum | Dealer Card | Action\")\n","    print(\"-\" * 35)\n","\n","    for player_sum, dealer_card, usable_ace in test_states:\n","        state = np.array([player_sum, dealer_card, usable_ace])\n","        with torch.no_grad():\n","            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n","            action = agent.policy_net(state_tensor).argmax().item()\n","            action_name = \"Hit\" if action == 1 else \"Stand\"\n","            print(f\"{player_sum:^10} | {dealer_card:^11} | {action_name:^6}\")\n","\n","# Run strategy analysis\n","analyze_strategy(NoisyDDQN)"]},{"cell_type":"markdown","metadata":{"id":"kVFaJR1rjcSx"},"source":["### Interactive Human Feedback Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3dEFiVDf_W9","outputId":"a0ff5ccb-1005-4f35-915c-eaa8f4d7273e","executionInfo":{"status":"ok","timestamp":1732763126993,"user_tz":480,"elapsed":19993,"user":{"displayName":"Bryan Alexis Ambriz","userId":"16154433038435291108"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting interactive learning blackjack game...\n","\n","Welcome to Interactive Learning Blackjack!\n","The AI will learn from your games.\n","\n","Dealer shows: [6, ?]\n","Your cards: [10, 8]\n","Your total: 18\n","\n","AI Recommends: Stand (Confidence: 0.76)\n","\n","Your action (H/S): S\n","Dealer hits: [6, 5, 3] (Total: 14)\n","Dealer hits: [6, 5, 3, Q] (Total: 24)\n","\n","Final hands:\n","Dealer: [6, 5, 3, Q] (Total: 24)\n","Player: [10, 8] (Total: 18)\n","Dealer busts! You win!\n","\n","Current Stats:\n","Games Played: 1\n","Win Rate: 100.0%\n","\n","Play again? (Y/N): N\n","\n","Final Stats:\n","Games Played: 1\n","Wins: 1\n","Losses: 0\n","Draws: 0\n","Win Rate: 100.0%\n","\n","Improved model saved!\n"]}],"source":["def card_name(card):\n","    \"\"\"Convert card number to readable format\"\"\"\n","    if card == 1:\n","        return 'A'\n","    elif card == 11:\n","        return 'J'\n","    elif card == 12:\n","        return 'Q'\n","    elif card == 13:\n","        return 'K'\n","    else:\n","        return str(card)\n","\n","def print_cards(cards, hidden=False):\n","    \"\"\"Display cards in readable format\"\"\"\n","    if hidden:\n","        return f\"[{card_name(cards[0])}, ?]\"\n","    return f\"[{', '.join(card_name(c) for c in cards)}]\"\n","\n","def get_card_value(card):\n","    if card == 1:  # Ace\n","        return 11\n","    return min(card, 10)\n","\n","def calculate_hand_value(cards):\n","    value = sum(get_card_value(card) for card in cards)\n","    num_aces = cards.count(1)\n","\n","    # Adjust for aces\n","    while value > 21 and num_aces:\n","        value -= 10\n","        num_aces -= 1\n","\n","    return value\n","\n","def play_interactive_blackjack_with_learning(agent):\n","    print(\"\\nWelcome to Interactive Learning Blackjack!\")\n","    print(\"The AI will learn from your games.\")\n","\n","    game_memory = []  # Store game experiences\n","    stats = {'games': 0, 'wins': 0, 'losses': 0, 'draws': 0}\n","\n","    while True:\n","        player_cards = []\n","        dealer_cards = []\n","        deck = list(range(1, 14)) * 4\n","        random.shuffle(deck)\n","\n","        # Initial deal\n","        player_cards.extend([deck.pop(), deck.pop()])\n","        dealer_cards.extend([deck.pop(), deck.pop()])\n","\n","        game_states = []  # Store states for this game\n","\n","        while True:\n","            print(\"\\nDealer shows:\", print_cards(dealer_cards, hidden=True))\n","            print(\"Your cards:\", print_cards(player_cards))\n","            player_value = calculate_hand_value(player_cards)\n","            print(f\"Your total: {player_value}\")\n","\n","            if player_value > 21:\n","                print(\"Bust! You lose.\")\n","                stats['losses'] += 1\n","                break\n","\n","            # Current state\n","            current_state = np.array([player_value, get_card_value(dealer_cards[0]), 1 in player_cards])\n","\n","            # Get AI recommendation\n","            with torch.no_grad():\n","                state_tensor = torch.FloatTensor(current_state).unsqueeze(0).to(agent.device)\n","                q_values = agent.policy_net(state_tensor)\n","                ai_action = q_values.argmax().item()\n","                confidence = torch.softmax(q_values, dim=1)[0]\n","                ai_recommendation = \"Hit\" if ai_action == 1 else \"Stand\"\n","                print(f\"\\nAI Recommends: {ai_recommendation} (Confidence: {confidence[ai_action]:.2f})\")\n","\n","            action = input(\"\\nYour action (H/S): \").upper()\n","            while action not in ['H', 'S']:\n","                action = input(\"Invalid input. Please enter H or S: \").upper()\n","\n","            # Store state and action\n","            game_states.append((\n","                current_state,\n","                1 if action == 'H' else 0,\n","                player_value\n","            ))\n","\n","            if action == 'H':\n","                player_cards.append(deck.pop())\n","            else:\n","                break\n","\n","        # Game ended, calculate final reward\n","        final_player_value = calculate_hand_value(player_cards)\n","        dealer_value = play_dealer_hand(dealer_cards, deck)\n","\n","        if final_player_value <= 21:\n","            print(\"\\nFinal hands:\")\n","            print(f\"Dealer: {print_cards(dealer_cards)} (Total: {dealer_value})\")\n","            print(f\"Player: {print_cards(player_cards)} (Total: {final_player_value})\")\n","\n","            if dealer_value > 21:\n","                print(\"Dealer busts! You win!\")\n","                reward = 1.0\n","                stats['wins'] += 1\n","            elif dealer_value > final_player_value:\n","                print(\"Dealer wins!\")\n","                reward = -1.0\n","                stats['losses'] += 1\n","            elif dealer_value < final_player_value:\n","                print(\"You win!\")\n","                reward = 1.0\n","                stats['wins'] += 1\n","            else:\n","                print(\"Push (tie)!\")\n","                reward = 0.0\n","                stats['draws'] += 1\n","        else:\n","            reward = -1.0\n","\n","        # Store experiences for learning\n","        for state, action, value in game_states:\n","            agent.memory.push(state, action, reward, state, True)\n","            # Train on a batch\n","            if len(agent.memory) >= agent.batch_size:\n","                loss = agent.train_step()\n","                if loss:\n","                    print(f\"Training loss: {loss:.4f}\")\n","\n","        stats['games'] += 1\n","        print(\"\\nCurrent Stats:\")\n","        print(f\"Games Played: {stats['games']}\")\n","        print(f\"Win Rate: {stats['wins']/stats['games']*100:.1f}%\")\n","\n","        play_again = input(\"\\nPlay again? (Y/N): \").upper()\n","        if play_again != 'Y':\n","            break\n","\n","    print(\"\\nFinal Stats:\")\n","    print(f\"Games Played: {stats['games']}\")\n","    print(f\"Wins: {stats['wins']}\")\n","    print(f\"Losses: {stats['losses']}\")\n","    print(f\"Draws: {stats['draws']}\")\n","    print(f\"Win Rate: {stats['wins']/stats['games']*100:.1f}%\")\n","\n","    # Save the improved model\n","    torch.save({\n","        'policy_net_state_dict': agent.policy_net.state_dict(),\n","        'optimizer_state_dict': agent.optimizer.state_dict(),\n","        'epsilon': agent.epsilon,\n","    }, 'blackjack_dqn_improved.pth')\n","    print(\"\\nImproved model saved!\")\n","\n","# Helper function for dealer's turn\n","def play_dealer_hand(dealer_cards, deck):\n","    dealer_value = calculate_hand_value(dealer_cards)\n","    while dealer_value < 17:\n","        dealer_cards.append(deck.pop())\n","        dealer_value = calculate_hand_value(dealer_cards)\n","        print(f\"Dealer hits: {print_cards(dealer_cards)} (Total: {dealer_value})\")\n","    return dealer_value\n","\n","# Start the learning interactive game\n","print(\"\\nStarting interactive learning blackjack game...\")\n","play_interactive_blackjack_with_learning(NoisyDDQN)"]},{"cell_type":"code","source":[],"metadata":{"id":"qy0m-Ynnudxz"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}